{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CoLABERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maraudier/SquadColaChat/blob/Maraudier-testing-1/CoLABERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEfSbAA4QHas",
        "colab_type": "code",
        "outputId": "b1e89e5b-4164-4a71-b16c-f7d6be37b45a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYsV4H8fCpZ-",
        "colab_type": "code",
        "outputId": "58399410-bf1a-455f-8a1c-d7e6290654d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('%d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('Using GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('Only CPU available')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 GPU(s) available.\n",
            "Using GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NmMdkZO8R6q",
        "colab_type": "code",
        "outputId": "4941b368-e993-42d7-9f2b-0be8bf4caa8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m6AnuFv0QXQ",
        "colab_type": "code",
        "outputId": "6b560ee1-795f-42b5-d8ce-009b99fa3c03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMtmPMkBzrvs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import wget\n",
        "import os\n",
        "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
        "if not os.path.exists('./cola_public_1.1.zip'):\n",
        "    wget.download(url, './cola_public_1.1.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Yv-tNv20dnH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists('./cola_public/'):\n",
        "    !unzip cola_public_1.1.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UkeC7SG2krJ",
        "colab_type": "code",
        "outputId": "7145ed7d-01db-44d4-dbc1-0513973aabd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        }
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', \n",
        "                 header=None, \n",
        "                 names=['sentence_source', \n",
        "                        'label', \n",
        "                        'label_notes', \n",
        "                        'sentence'])\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "#Visual Education\n",
        "print(\"Training samples: {:,}\\n\".format(df.shape[0]))\n",
        "print(\"Example of label 0 = Unacceptable sentances \")\n",
        "display(df.loc[df.label == 0].sample(3)[['sentence', 'label']])\n",
        "print(\"\\nExample of label 1 = Acceptable sentances\")\n",
        "display(df.loc[df.label == 1].sample(3)[['sentence', 'label']])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training samples: 8,551\n",
            "\n",
            "Example of label 0 = Unacceptable sentances \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4021</th>\n",
              "      <td>The foxes seem compatible for the chickens.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4099</th>\n",
              "      <td>Why don't you leaving me concentrate on my work?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>The professor talked us.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              sentence  label\n",
              "4021       The foxes seem compatible for the chickens.      0\n",
              "4099  Why don't you leaving me concentrate on my work?      0\n",
              "20                            The professor talked us.      0"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Example of label 1 = Acceptable sentances\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Bill broke the bathtub into pieces.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2229</th>\n",
              "      <td>This machine records well.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6401</th>\n",
              "      <td>Every student in Mary's class, by virtue of be...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  label\n",
              "15                  Bill broke the bathtub into pieces.      1\n",
              "2229                         This machine records well.      1\n",
              "6401  Every student in Mary's class, by virtue of be...      1"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z474sSC6oe7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bBdb3pt8LuQ",
        "colab_type": "code",
        "outputId": "21fa968c-8522-4f31-ba63-fa2348e88bf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "input_ids = []\n",
        "\n",
        "# For every sentence... Define sent from SQAUD Dataset\n",
        "#df.loc[df.label == 0].sample(5)[['sentence', 'label']]\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Token IDs: [101, 2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhUZO9vc_l6T",
        "colab_type": "code",
        "outputId": "0f3b1499-0e3c-40f1-cced-766590d48ebc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  47\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHXSsKimGZQ0",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp-54FcQ_p3h",
        "colab_type": "text"
      },
      "source": [
        "Given that, let's choose MAX_LEN = 64 and apply the padding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cp9BPRd1tMIo",
        "colab_type": "code",
        "outputId": "08f0862a-a565-49c1-a5d3-db3ce2cf5a16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "MAX_LEN = 50\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')\n",
        "\n",
        "print(input_ids[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 50 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n",
            "[  101  2256  2814  2180  1005  1056  4965  2023  4106  1010  2292  2894\n",
            "  1996  2279  2028  2057 16599  1012   102     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDoC24LeEv3N",
        "colab_type": "code",
        "outputId": "7abdf247-4c84-4cee-a0f3-5079da9510f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "attention_masks = []\n",
        "\n",
        "for sent in input_ids:\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    attention_masks.append(att_mask)\n",
        "\n",
        "print(attention_masks[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "trpQ2X-8Y_h3",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#80/20\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.2)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw5K2A5Ko1RF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEgLpFVlo1Z-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFsCTp_mporB",
        "colab_type": "code",
        "outputId": "877315e0-f7f2-4823-99ee-bb060453a26b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", \n",
        "    num_labels = 2, \n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False)\n",
        "\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PIiVlDYCtSq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "# print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "# print('==== Embedding Layer ====\\n')\n",
        "\n",
        "# for p in params[0:5]:\n",
        "#     print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "# print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "# for p in params[5:21]:\n",
        "#     print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "# print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "# for p in params[-4:]:\n",
        "#     print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLs72DuMODJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 3e-5,\n",
        "                  eps = 1e-8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-p0upAhhRiIx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "epochs = 2\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cQNvaZ9bnyy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpt6tR83keZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J-FYdx6nFE_",
        "colab_type": "code",
        "outputId": "0f3f545d-7d30-4c2c-d415-4562c0a80e1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "import random\n",
        "\n",
        "seed_val = 38\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "loss_values = []\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    t0 = time.time()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "    t0 = time.time()\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    428.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    428.    Elapsed: 0:00:14.\n",
            "  Batch   120  of    428.    Elapsed: 0:00:21.\n",
            "  Batch   160  of    428.    Elapsed: 0:00:28.\n",
            "  Batch   200  of    428.    Elapsed: 0:00:34.\n",
            "  Batch   240  of    428.    Elapsed: 0:00:41.\n",
            "  Batch   280  of    428.    Elapsed: 0:00:49.\n",
            "  Batch   320  of    428.    Elapsed: 0:00:56.\n",
            "  Batch   360  of    428.    Elapsed: 0:01:03.\n",
            "  Batch   400  of    428.    Elapsed: 0:01:10.\n",
            "\n",
            "  Average training loss: 0.50\n",
            "  Training epcoh took: 0:00:00\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n",
            "  Batch    40  of    428.    Elapsed: 0:00:07.\n",
            "  Batch    80  of    428.    Elapsed: 0:00:15.\n",
            "  Batch   120  of    428.    Elapsed: 0:00:22.\n",
            "  Batch   160  of    428.    Elapsed: 0:00:29.\n",
            "  Batch   200  of    428.    Elapsed: 0:00:37.\n",
            "  Batch   240  of    428.    Elapsed: 0:00:44.\n",
            "  Batch   280  of    428.    Elapsed: 0:00:51.\n",
            "  Batch   320  of    428.    Elapsed: 0:00:59.\n",
            "  Batch   360  of    428.    Elapsed: 0:01:06.\n",
            "  Batch   400  of    428.    Elapsed: 0:01:13.\n",
            "\n",
            "  Average training loss: 0.27\n",
            "  Training epcoh took: 0:00:00\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation took: 0:00:06\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-G03mmwH3aI",
        "colab_type": "text"
      },
      "source": [
        "Let's take a look at our training loss over all batches:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68xreA9JAmG5",
        "colab_type": "code",
        "outputId": "33320e17-52a2-4616-ea7f-b35984ca6e97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvAAAAGaCAYAAABpIXfbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKVChXWBsVAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAN0LZBOOPVh",
        "colab_type": "code",
        "outputId": "32bfb669-8a2f-47c3-db14-fb75385b7d8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 516\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hba10sXR7Xi6",
        "colab_type": "code",
        "outputId": "d5b1b39f-bbd4-484f-fe46-18c5bf730da3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  # print(b_input_ids.shape)\n",
        "  \n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "  # print(type(logits))\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  # print(logits)\n",
        "  # print(label_ids)\n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 516 test sentences...\n",
            "[[-1.538613    0.79492474]\n",
            " [-1.9691813   1.2737497 ]\n",
            " [-2.0917194   1.5434027 ]\n",
            " [-2.2420774   1.5707169 ]\n",
            " [-1.9667499   1.1803105 ]\n",
            " [-2.4949188   1.962707  ]\n",
            " [-1.5878145   0.920532  ]\n",
            " [-2.0202684   1.4587833 ]\n",
            " [-2.592879    2.1435487 ]\n",
            " [-2.1405864   1.7431492 ]\n",
            " [-1.7049026   1.0377374 ]\n",
            " [ 0.03699965 -0.10038966]\n",
            " [-2.1580524   1.2887192 ]\n",
            " [-0.8708725   0.1979861 ]\n",
            " [-1.9869808   1.340426  ]\n",
            " [-1.746131    0.8906261 ]\n",
            " [-2.0602882   1.3783175 ]\n",
            " [ 1.5511489  -1.3508545 ]\n",
            " [-1.7716842   1.3031995 ]\n",
            " [-1.1232085   1.0699772 ]\n",
            " [-1.7716842   1.3031995 ]\n",
            " [-1.5867031   0.9369427 ]\n",
            " [-1.8288385   1.3004646 ]\n",
            " [-2.1540415   1.539912  ]\n",
            " [-1.6933258   0.9525095 ]\n",
            " [-1.8595952   1.1120687 ]\n",
            " [-1.9480929   1.5139014 ]\n",
            " [-1.9154068   1.3498212 ]\n",
            " [-2.2230394   1.5007023 ]\n",
            " [-1.6633259   1.0462829 ]\n",
            " [-2.001639    1.3397664 ]\n",
            " [-1.5236704   0.8351253 ]]\n",
            "[1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 1]\n",
            "[[-0.43420994  0.3225862 ]\n",
            " [-1.3613538   0.6266975 ]\n",
            " [-0.8587187   0.74832934]\n",
            " [-2.005765    1.589664  ]\n",
            " [-1.4020548   0.7400423 ]\n",
            " [-1.8891399   1.4933709 ]\n",
            " [-2.2527843   1.5316559 ]\n",
            " [-2.2874126   1.6447264 ]\n",
            " [-0.31087935  0.04003145]\n",
            " [-1.2198162   1.057319  ]\n",
            " [-1.4562485   1.0151682 ]\n",
            " [ 0.6161194  -0.58675045]\n",
            " [-1.8943243   1.2635037 ]\n",
            " [-1.1199474   0.34912804]\n",
            " [-1.0305705   0.27809456]\n",
            " [-1.0875419   0.4420649 ]\n",
            " [-1.897276    1.138088  ]\n",
            " [-2.1176383   1.4122123 ]\n",
            " [-2.0883045   1.4134065 ]\n",
            " [-1.9640765   1.1947232 ]\n",
            " [-0.17940827 -0.2776557 ]\n",
            " [ 1.4789063  -1.3179348 ]\n",
            " [-2.1668136   1.5931752 ]\n",
            " [-2.1236057   1.4253106 ]\n",
            " [-2.0264761   1.5066003 ]\n",
            " [-2.0264761   1.5066003 ]\n",
            " [-2.2298582   1.6698517 ]\n",
            " [-1.3188933   0.7370995 ]\n",
            " [-1.5583994   0.94255817]\n",
            " [-1.4432523   1.2625374 ]\n",
            " [-1.3188933   0.7370995 ]\n",
            " [-2.1523368   1.439908  ]]\n",
            "[1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 0 1]\n",
            "[[-2.1564493   1.5009528 ]\n",
            " [-2.0931797   1.4971457 ]\n",
            " [-1.9335458   1.1998217 ]\n",
            " [-2.011336    1.3538418 ]\n",
            " [-2.085447    1.4992568 ]\n",
            " [-1.4560544   0.8050666 ]\n",
            " [ 0.53373164 -0.5635269 ]\n",
            " [-0.36587697 -0.04087891]\n",
            " [-2.2838583   1.8043565 ]\n",
            " [-2.1180048   1.5859674 ]\n",
            " [-2.2855027   1.7748336 ]\n",
            " [-1.8148595   1.119788  ]\n",
            " [-1.6431441   0.8849092 ]\n",
            " [ 1.4815081  -1.2830887 ]\n",
            " [-1.7037363   0.99656963]\n",
            " [-1.1725997   0.4468532 ]\n",
            " [-1.5858351   0.8873703 ]\n",
            " [ 1.3472738  -1.2303052 ]\n",
            " [-2.215426    1.6197063 ]\n",
            " [-2.439572    1.8420647 ]\n",
            " [-1.8154135   1.3589631 ]\n",
            " [-2.5684545   2.226084  ]\n",
            " [ 1.6694304  -1.5068648 ]\n",
            " [-0.24862687 -0.00415246]\n",
            " [-2.0774977   1.4756559 ]\n",
            " [-2.1230097   1.5258802 ]\n",
            " [-2.179975    1.7005389 ]\n",
            " [ 1.3851445  -1.2428939 ]\n",
            " [ 0.9301885  -0.8974301 ]\n",
            " [ 1.7024636  -1.4945326 ]\n",
            " [-2.168309    1.5593954 ]\n",
            " [ 1.7218658  -1.5514865 ]]\n",
            "[1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 0 1 0]\n",
            "[[-2.2324502   1.598308  ]\n",
            " [ 1.3840744  -1.1396852 ]\n",
            " [ 1.6605327  -1.4686807 ]\n",
            " [-0.32046404  0.13325596]\n",
            " [-2.1454813   1.538835  ]\n",
            " [-2.487919    1.9686345 ]\n",
            " [-2.2568192   1.6453147 ]\n",
            " [-0.89198786  0.34894535]\n",
            " [-2.4394658   1.9053804 ]\n",
            " [-1.5194902   0.88049734]\n",
            " [-1.7385832   1.1034819 ]\n",
            " [-1.8462253   1.3937547 ]\n",
            " [-2.2174265   1.5583347 ]\n",
            " [-1.7173569   1.1097566 ]\n",
            " [-1.3402503   0.7401121 ]\n",
            " [-2.0211744   1.5221487 ]\n",
            " [-2.3919497   1.9687272 ]\n",
            " [-2.1393712   1.6784891 ]\n",
            " [-2.099586    1.5980418 ]\n",
            " [-2.172198    1.6831417 ]\n",
            " [-2.251149    1.5562825 ]\n",
            " [-2.0145805   1.2530739 ]\n",
            " [-0.25280836 -0.04181635]\n",
            " [-1.361666    0.6749112 ]\n",
            " [-2.256248    1.6842337 ]\n",
            " [-2.3996916   1.9496769 ]\n",
            " [-2.459767    1.9146454 ]\n",
            " [-1.3596913   0.83135426]\n",
            " [ 0.06126544 -0.01069425]\n",
            " [ 0.55950624 -0.5452408 ]\n",
            " [-2.0870004   1.4079221 ]\n",
            " [ 0.97583765 -0.8019117 ]]\n",
            "[1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1]\n",
            "[[-0.68752074  0.11617765]\n",
            " [ 0.45600998 -0.31651497]\n",
            " [-2.170154    1.614383  ]\n",
            " [ 0.803794   -0.7198083 ]\n",
            " [-1.4770392   0.8868606 ]\n",
            " [-2.0822675   1.6283507 ]\n",
            " [-0.08091492 -0.04970936]\n",
            " [-2.4187384   1.9003775 ]\n",
            " [-1.6988001   1.0634873 ]\n",
            " [-1.8332648   1.0942738 ]\n",
            " [-1.2555532   1.0401216 ]\n",
            " [-2.2968538   1.7710732 ]\n",
            " [-1.4800031   1.3601618 ]\n",
            " [-1.4233506   1.0521042 ]\n",
            " [-2.2838392   1.3909893 ]\n",
            " [-1.1222787   1.0923437 ]\n",
            " [-1.4870527   0.72223645]\n",
            " [-0.01581127  0.03094789]\n",
            " [-1.6025417   0.81286657]\n",
            " [-1.6671051   0.9132848 ]\n",
            " [-0.04291413  0.07172924]\n",
            " [ 0.8439213  -0.70883435]\n",
            " [-1.5510535   0.73715305]\n",
            " [-0.4691373   0.41056862]\n",
            " [-1.793328    1.1461465 ]\n",
            " [-2.5035956   2.0806167 ]\n",
            " [-2.1420894   1.5417402 ]\n",
            " [-2.4610803   1.8160639 ]\n",
            " [-2.215794    1.6285173 ]\n",
            " [-1.8141134   1.1759701 ]\n",
            " [ 1.0264924  -0.89161193]\n",
            " [-2.219609    1.5001594 ]]\n",
            "[1 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 0 1]\n",
            "[[-0.97165203  0.76610595]\n",
            " [-2.0630872   1.3145245 ]\n",
            " [-1.9775894   1.228366  ]\n",
            " [-1.979163    1.2796843 ]\n",
            " [-2.17326     1.6291326 ]\n",
            " [ 1.614756   -1.4070884 ]\n",
            " [ 1.6716237  -1.4923348 ]\n",
            " [ 1.3304662  -1.1130689 ]\n",
            " [ 1.5638664  -1.4140818 ]\n",
            " [ 1.0039392  -0.78513384]\n",
            " [-2.0681102   1.5962709 ]\n",
            " [-1.3560541   0.6355461 ]\n",
            " [-0.93419015  0.7238029 ]\n",
            " [ 1.3255548  -1.0640298 ]\n",
            " [-2.1381595   1.5862057 ]\n",
            " [ 1.368214   -1.1416779 ]\n",
            " [-1.92232     1.2412713 ]\n",
            " [-1.8551707   1.2281681 ]\n",
            " [-2.3491476   1.8343546 ]\n",
            " [-2.3535905   1.7381597 ]\n",
            " [ 1.6840882  -1.5267731 ]\n",
            " [ 1.6365085  -1.4650788 ]\n",
            " [ 1.3243703  -1.1589221 ]\n",
            " [-1.9546342   1.2469674 ]\n",
            " [-2.378487    1.6440482 ]\n",
            " [-2.2084835   1.6813829 ]\n",
            " [-2.440532    1.8748795 ]\n",
            " [-0.25961274  0.08682216]\n",
            " [-2.2546241   1.5592391 ]\n",
            " [-2.4933784   1.7757095 ]\n",
            " [-2.21563     1.5474715 ]\n",
            " [-0.8650436   0.28964797]]\n",
            "[1 1 1 1 1 1 0 0 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 1 1 1 0 1 1 1 0]\n",
            "[[-2.250868    1.7294011 ]\n",
            " [ 1.4886142  -1.2746512 ]\n",
            " [-2.2961712   1.6860557 ]\n",
            " [-2.3589468   1.7152673 ]\n",
            " [-0.5317641   0.39368656]\n",
            " [-1.9192297   1.2598122 ]\n",
            " [-0.08091753 -0.01013402]\n",
            " [-2.18269     1.3114823 ]\n",
            " [-1.2645977   0.8717692 ]\n",
            " [-1.835505    1.3372926 ]\n",
            " [ 1.2325702  -1.1205527 ]\n",
            " [ 1.1697668  -1.064903  ]\n",
            " [-1.8848581   1.3537283 ]\n",
            " [-2.435715    2.0429323 ]\n",
            " [ 1.2340839  -1.0962768 ]\n",
            " [-1.8073213   1.1765298 ]\n",
            " [-2.5248039   1.8949575 ]\n",
            " [-2.278564    1.7638078 ]\n",
            " [-2.1884837   1.585279  ]\n",
            " [-2.2165651   1.6816924 ]\n",
            " [-2.531466    1.8418367 ]\n",
            " [ 1.0412068  -0.90640813]\n",
            " [-0.18661945  0.07495306]\n",
            " [-2.1105773   1.3905596 ]\n",
            " [-1.9038212   1.1984601 ]\n",
            " [-2.4022827   1.8990446 ]\n",
            " [-2.3721614   1.8215936 ]\n",
            " [ 0.977477   -0.9823373 ]\n",
            " [-2.1021776   1.615996  ]\n",
            " [-1.5468316   1.0753524 ]\n",
            " [ 1.3214045  -1.1242585 ]\n",
            " [-2.539593    1.941839  ]]\n",
            "[1 0 1 1 1 1 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0 0 1]\n",
            "[[ 0.41688532 -0.3458047 ]\n",
            " [-2.5109315   1.8757963 ]\n",
            " [-2.5208552   1.8721311 ]\n",
            " [-0.8555687   0.58457524]\n",
            " [-2.185033    1.5440309 ]\n",
            " [-2.2505584   1.6948555 ]\n",
            " [-0.40527815  0.08038534]\n",
            " [-2.2213178   1.5791527 ]\n",
            " [-1.9851317   1.3506843 ]\n",
            " [-2.6013021   2.1068847 ]\n",
            " [-2.5054166   2.051243  ]\n",
            " [-2.347716    1.7730116 ]\n",
            " [-2.4001281   1.9081379 ]\n",
            " [-2.6323712   1.992514  ]\n",
            " [-2.54762     1.944096  ]\n",
            " [-2.4842796   1.8461801 ]\n",
            " [-2.4425516   1.8585802 ]\n",
            " [-2.0689745   1.4332659 ]\n",
            " [-2.0279486   1.4738917 ]\n",
            " [-2.5850265   1.7857215 ]\n",
            " [-2.3917718   1.9167234 ]\n",
            " [-2.2980247   1.7444527 ]\n",
            " [-2.0354419   1.3143746 ]\n",
            " [-2.5797594   1.9028786 ]\n",
            " [-2.451267    2.2343268 ]\n",
            " [-2.2471483   1.6327019 ]\n",
            " [-2.4452975   1.9178575 ]\n",
            " [-2.5081568   1.98818   ]\n",
            " [-0.9980183   0.53434426]\n",
            " [-1.8791705   1.2365824 ]\n",
            " [-0.7706244   0.3859347 ]\n",
            " [-2.196454    1.6581577 ]]\n",
            "[0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "[[ 1.0264245  -1.0104897 ]\n",
            " [-2.30015     1.5735866 ]\n",
            " [ 1.1723847  -1.1583799 ]\n",
            " [-2.513603    1.9374977 ]\n",
            " [ 0.9914492  -0.85428244]\n",
            " [-0.24376771  0.18122919]\n",
            " [-2.666956    1.9838474 ]\n",
            " [-2.5743628   2.1404886 ]\n",
            " [-2.4234948   1.819694  ]\n",
            " [-2.4954917   1.8913212 ]\n",
            " [-2.5956879   2.0598493 ]\n",
            " [-2.5368369   1.9812542 ]\n",
            " [-2.4932036   1.9485378 ]\n",
            " [-2.5552733   2.0269246 ]\n",
            " [-2.5318553   1.9229765 ]\n",
            " [-2.473367    1.8550264 ]\n",
            " [-2.5708447   2.0454435 ]\n",
            " [-2.4692647   1.9458799 ]\n",
            " [-2.5284264   1.8877891 ]\n",
            " [-2.5015137   1.958445  ]\n",
            " [-2.6318145   2.1222484 ]\n",
            " [-1.7701432   1.0787901 ]\n",
            " [-2.270639    1.6316781 ]\n",
            " [-1.631124    0.92167556]\n",
            " [-0.82210994  0.1129688 ]\n",
            " [-2.587145    2.0854502 ]\n",
            " [ 1.7324524  -1.5096135 ]\n",
            " [ 1.6744508  -1.4652427 ]\n",
            " [-2.631541    2.0378196 ]\n",
            " [ 0.8665933  -0.9198189 ]\n",
            " [-1.1964499   0.30761322]\n",
            " [-2.455193    1.9021763 ]]\n",
            "[0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 0 1]\n",
            "[[ 1.0109884  -0.8935554 ]\n",
            " [ 0.07419696 -0.20846963]\n",
            " [-2.4274762   2.169652  ]\n",
            " [ 1.4244343  -1.3171272 ]\n",
            " [ 1.5884666  -1.382151  ]\n",
            " [-2.0137534   1.3985966 ]\n",
            " [-2.2206984   1.6967175 ]\n",
            " [-2.4665427   2.047114  ]\n",
            " [ 0.9586952  -0.87113374]\n",
            " [ 0.2800814  -0.36611637]\n",
            " [-2.0810819   1.4143327 ]\n",
            " [-2.4140244   2.0055056 ]\n",
            " [ 0.9640226  -0.9512747 ]\n",
            " [ 1.2266461  -1.2156701 ]\n",
            " [ 0.19366276 -0.45070282]\n",
            " [-2.6285114   2.1108165 ]\n",
            " [-2.6258793   2.126427  ]\n",
            " [ 1.6323318  -1.4651012 ]\n",
            " [-2.5973048   2.1465654 ]\n",
            " [-2.572488    2.3218668 ]\n",
            " [-1.6257844   1.111203  ]\n",
            " [ 1.6395347  -1.4359821 ]\n",
            " [-2.4055266   1.9332039 ]\n",
            " [-2.5262864   2.0119872 ]\n",
            " [ 1.1048627  -1.049838  ]\n",
            " [ 0.98903537 -1.0138419 ]\n",
            " [ 0.3339519  -0.45583743]\n",
            " [-1.9352994   1.3345358 ]\n",
            " [-2.385379    1.8442149 ]\n",
            " [-0.9373709   0.25169513]\n",
            " [-1.5414014   0.8165968 ]\n",
            " [-2.2881663   1.7166471 ]]\n",
            "[0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 1 1 0 0 0 1 1 1 0 1]\n",
            "[[-2.4228768e+00  1.9412841e+00]\n",
            " [ 1.1272150e+00 -9.2989135e-01]\n",
            " [-2.0771697e+00  1.5956879e+00]\n",
            " [ 1.5769553e+00 -1.3644258e+00]\n",
            " [-2.2913496e+00  1.9517975e+00]\n",
            " [ 6.9069201e-01 -6.9391590e-01]\n",
            " [-2.5772665e+00  2.1055796e+00]\n",
            " [-2.3875573e+00  1.9090899e+00]\n",
            " [-2.5696497e+00  2.1590583e+00]\n",
            " [-1.3808416e+00  7.4621391e-01]\n",
            " [-1.6423219e+00  1.0821632e+00]\n",
            " [-1.8356032e+00  1.2846446e+00]\n",
            " [-1.7024739e-01  1.6471520e-03]\n",
            " [ 6.7753935e-01 -5.7961285e-01]\n",
            " [-2.4904416e+00  2.2756984e+00]\n",
            " [-2.0338376e+00  1.3377284e+00]\n",
            " [-2.3029921e+00  1.8976904e+00]\n",
            " [-3.2860979e-01  1.7634030e-01]\n",
            " [-2.5223420e+00  2.0561767e+00]\n",
            " [-2.3666611e+00  1.7947502e+00]\n",
            " [-2.4450233e+00  2.1767919e+00]\n",
            " [ 1.2029262e+00 -1.0487065e+00]\n",
            " [-1.8959770e-01  5.5984333e-02]\n",
            " [ 4.3247037e-02 -4.5754761e-04]\n",
            " [-1.5018806e+00  1.5148232e+00]\n",
            " [-2.0566697e+00  1.4320319e+00]\n",
            " [-2.0815363e+00  1.2993687e+00]\n",
            " [-2.5657864e+00  2.0045445e+00]\n",
            " [-2.4428442e+00  1.8527138e+00]\n",
            " [-2.6201417e+00  2.2359231e+00]\n",
            " [ 3.6281613e-01 -4.3348652e-01]\n",
            " [-2.5228384e+00  1.9282279e+00]]\n",
            "[1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1]\n",
            "[[-2.3996916e+00  1.8092909e+00]\n",
            " [ 8.2418673e-02  7.0385933e-02]\n",
            " [-2.5777960e+00  1.9658047e+00]\n",
            " [-2.4080007e+00  1.7891632e+00]\n",
            " [-2.3988240e+00  1.8315876e+00]\n",
            " [-2.3853176e+00  1.8438433e+00]\n",
            " [-2.4640532e+00  1.9276379e+00]\n",
            " [-2.5294850e+00  2.2597613e+00]\n",
            " [ 1.6525760e+00 -1.4621409e+00]\n",
            " [-2.4224823e+00  1.9321622e+00]\n",
            " [ 1.1273619e+00 -1.0264870e+00]\n",
            " [ 8.1697613e-01 -9.3017602e-01]\n",
            " [ 9.3446118e-01 -7.8559411e-01]\n",
            " [-2.5221262e+00  2.0727761e+00]\n",
            " [-2.7006965e+00  2.3123612e+00]\n",
            " [-2.6775386e+00  2.2998071e+00]\n",
            " [-2.4967005e+00  2.2113280e+00]\n",
            " [-2.3161993e+00  1.6436837e+00]\n",
            " [-7.6697510e-01  3.9184380e-01]\n",
            " [-2.5265939e+00  2.0262716e+00]\n",
            " [ 1.0541865e+00 -8.4010231e-01]\n",
            " [ 1.6966329e-03 -4.4480100e-02]\n",
            " [-1.8129199e+00  1.1652378e+00]\n",
            " [-1.6863239e+00  9.9985087e-01]\n",
            " [-2.4045393e+00  1.8531444e+00]\n",
            " [-1.1287483e+00  4.0491840e-01]\n",
            " [-2.4116340e+00  1.9563212e+00]\n",
            " [ 1.5561243e+00 -1.3535943e+00]\n",
            " [ 1.6838925e+00 -1.4875468e+00]\n",
            " [-2.5677333e+00  2.0968564e+00]\n",
            " [-2.5409367e+00  2.1504111e+00]\n",
            " [-2.5881326e+00  2.2370665e+00]]\n",
            "[1 0 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1]\n",
            "[[ 1.6305735  -1.3924917 ]\n",
            " [ 1.5403091  -1.3649144 ]\n",
            " [-1.3554789   0.8196251 ]\n",
            " [-2.3252132   1.9821863 ]\n",
            " [ 1.0476404  -0.8674472 ]\n",
            " [-2.2528005   1.5632957 ]\n",
            " [-1.9823384   1.299956  ]\n",
            " [-2.2097168   1.5454992 ]\n",
            " [ 1.1722142  -1.0323404 ]\n",
            " [-2.5385551   2.1299567 ]\n",
            " [ 1.3030248  -1.2706382 ]\n",
            " [ 1.5670379  -1.3785311 ]\n",
            " [ 0.69462734 -0.7920487 ]\n",
            " [-2.599489    2.1934857 ]\n",
            " [-2.6118467   2.1435616 ]\n",
            " [ 1.1900822  -1.0709903 ]\n",
            " [-2.2740629   1.5878543 ]\n",
            " [-2.5231497   2.0569777 ]\n",
            " [-2.6120427   2.107922  ]\n",
            " [ 1.6564494  -1.4894096 ]\n",
            " [-2.4992692   1.9049871 ]\n",
            " [-2.141648    1.5784938 ]\n",
            " [-2.3870468   1.8117509 ]\n",
            " [ 1.6039023  -1.3850003 ]\n",
            " [-2.4902809   2.0078025 ]\n",
            " [-2.5850735   2.0414715 ]\n",
            " [-2.5204723   2.0875509 ]\n",
            " [ 1.6625307  -1.4977926 ]\n",
            " [ 1.3417844  -1.1786796 ]\n",
            " [-2.2373924   1.7231787 ]\n",
            " [-2.5319498   1.8860569 ]\n",
            " [-2.4064937   1.8629689 ]]\n",
            "[0 0 0 1 0 1 1 1 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1]\n",
            "[[-2.1985788e+00  1.5191423e+00]\n",
            " [-2.2919822e+00  1.7668079e+00]\n",
            " [-2.4472640e+00  1.8935890e+00]\n",
            " [-2.2841432e+00  1.6311702e+00]\n",
            " [ 1.4231504e+00 -1.2805587e+00]\n",
            " [ 1.2369705e+00 -1.1731560e+00]\n",
            " [-2.5270827e+00  1.8644866e+00]\n",
            " [-2.5445597e+00  1.9127100e+00]\n",
            " [-2.4587200e+00  1.9829543e+00]\n",
            " [-2.3734608e+00  1.8691803e+00]\n",
            " [ 5.1132747e-04 -2.9937720e-02]\n",
            " [-1.6481342e+00  1.0432782e+00]\n",
            " [-1.8537588e+00  1.1276879e+00]\n",
            " [-1.7988710e+00  1.2115458e+00]\n",
            " [-2.4940863e+00  1.9625065e+00]\n",
            " [-2.2033758e+00  1.6656145e+00]\n",
            " [-2.0325341e+00  1.4454663e+00]\n",
            " [-2.5259850e+00  1.9752433e+00]\n",
            " [-2.5429082e+00  2.0995302e+00]\n",
            " [-1.2040732e+00  6.8424827e-01]\n",
            " [ 1.5507890e+00 -1.3337711e+00]\n",
            " [-1.8018744e+00  1.1953746e+00]\n",
            " [-2.3960302e+00  1.9486133e+00]\n",
            " [-2.2254567e+00  1.6292117e+00]\n",
            " [-1.1516460e+00  3.4469447e-01]\n",
            " [ 6.5275192e-01 -7.4085349e-01]\n",
            " [-2.5369678e+00  2.0240009e+00]\n",
            " [-2.3827188e+00  1.7783438e+00]\n",
            " [-1.6017427e+00  1.0676483e+00]\n",
            " [ 1.4313505e+00 -1.2578923e+00]\n",
            " [ 1.7089483e+00 -1.5511394e+00]\n",
            " [ 1.6629713e+00 -1.5082875e+00]]\n",
            "[1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0]\n",
            "[[-2.3050249   1.740082  ]\n",
            " [-2.3015273   1.7387125 ]\n",
            " [-2.5735278   2.126998  ]\n",
            " [-2.4237716   2.01092   ]\n",
            " [ 1.5984476  -1.447647  ]\n",
            " [-2.3089507   1.7577336 ]\n",
            " [-2.548824    2.014961  ]\n",
            " [-2.4706578   1.9545226 ]\n",
            " [-2.2795293   1.8496625 ]\n",
            " [-1.064968    0.4422725 ]\n",
            " [ 1.0289402  -0.93874395]\n",
            " [-1.2359631   0.7304947 ]\n",
            " [-2.4049866   2.0085618 ]\n",
            " [-2.4128265   1.8331305 ]\n",
            " [-2.299581    1.874584  ]\n",
            " [-2.099877    1.4785962 ]\n",
            " [ 0.72302175 -0.5885376 ]\n",
            " [ 1.7286144  -1.5257046 ]\n",
            " [-2.439211    1.8901036 ]\n",
            " [-2.59012     2.0914545 ]\n",
            " [-0.07647288 -0.00432037]\n",
            " [-2.3525727   1.8599961 ]\n",
            " [-2.5099764   2.0473218 ]\n",
            " [-2.4250097   1.9094464 ]\n",
            " [ 0.66064864 -0.6045509 ]\n",
            " [ 0.5615159  -0.44175655]\n",
            " [ 0.47815308 -0.38273975]\n",
            " [-2.4361136   1.8741285 ]\n",
            " [-1.7357757   1.1847826 ]\n",
            " [ 0.307661   -0.22247885]\n",
            " [-1.8257992   1.2033952 ]\n",
            " [-2.0667493   1.3410211 ]]\n",
            "[1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1]\n",
            "[[ 1.1807733  -1.1821144 ]\n",
            " [-2.583739    1.922095  ]\n",
            " [ 1.5836504  -1.3953893 ]\n",
            " [ 1.2794099  -1.218196  ]\n",
            " [-2.2548048   1.677706  ]\n",
            " [ 1.6168412  -1.4649749 ]\n",
            " [ 1.175058   -0.9668372 ]\n",
            " [ 1.2722721  -1.063562  ]\n",
            " [-2.2316856   1.6648362 ]\n",
            " [ 1.323419   -1.1941767 ]\n",
            " [ 1.3271229  -1.2740422 ]\n",
            " [-2.3046281   1.5594786 ]\n",
            " [ 0.70542014 -0.7705296 ]\n",
            " [ 1.5547289  -1.370423  ]\n",
            " [-1.3748841   0.5421241 ]\n",
            " [-2.2604373   1.6903505 ]\n",
            " [-1.4336014   0.8960888 ]\n",
            " [-1.4735005   0.9082943 ]\n",
            " [ 1.2244189  -1.0218915 ]\n",
            " [-2.0626678   1.4882889 ]\n",
            " [-2.314711    1.7916311 ]\n",
            " [-0.14553374 -0.0445826 ]\n",
            " [-2.4563682   2.065411  ]\n",
            " [-0.00655264 -0.10899372]\n",
            " [ 0.98591816 -0.7468117 ]\n",
            " [-1.8052297   1.3337915 ]\n",
            " [-2.193414    1.4884157 ]\n",
            " [-1.9617373   1.2599627 ]\n",
            " [-1.5181038   0.79668206]\n",
            " [-1.9799469   1.3160193 ]\n",
            " [-2.0671592   1.4737395 ]\n",
            " [-2.4764185   1.8220648 ]]\n",
            "[1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1]\n",
            "[[-2.3723323   1.6564443 ]\n",
            " [-2.5326695   1.9917778 ]\n",
            " [-2.030703    1.388656  ]\n",
            " [-0.95790356  0.34032482]]\n",
            "[1 0 1 1]\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NVXaUJGjf_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWcy0X1hirdx",
        "colab_type": "code",
        "outputId": "d0fabe14-8c2f-486b-dc08-6aa640002748",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive samples: 354 of 516 (68.60%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRaZQ4XC7kLs",
        "colab_type": "code",
        "outputId": "50bbc27e-82ba-47d0-9ea3-c9edb0713a03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "  matthews_set.append(matthews)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUM0UA1qJaVB",
        "colab_type": "text"
      },
      "source": [
        "The final score will be based on the entire test set, but let's take a look at the scores on the individual batches to get a sense of the variability in the metric between batches. \n",
        "\n",
        "Each batch has 32 sentences in it, except the last batch which has only (516 % 32) = 4 test sentences in it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xytAr_C48wnu",
        "colab_type": "code",
        "outputId": "a8917631-4329-4088-e33b-781a3d99374c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "matthews_set"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.049286405809014416,\n",
              " -0.21684543705982773,\n",
              " 0.4040950971038548,\n",
              " 0.11365840154561273,\n",
              " 0.32328707534629597,\n",
              " 0.6777932975034471,\n",
              " 0.6831300510639732,\n",
              " 0.47519096331149147,\n",
              " 0.8320502943378436,\n",
              " 0.8126375006351424,\n",
              " 0.6777749493656265,\n",
              " 0.7624437362098716,\n",
              " 0.936441710371274,\n",
              " 0.647150228929434,\n",
              " 0.4622501635210242,\n",
              " 0.47306844125299624,\n",
              " 0.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCYZa1lQ8Jn8",
        "colab_type": "code",
        "outputId": "3eea8beb-6963-4bad-83c2-549dff4b54a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('Matthews correlation coefficient is measure of the quality of binary and multiclass classifications: %.3f' % mcc)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matthews correlation coefficient is measure of the quality of binary and multiclass classifications: 0.531\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mks5B6BO-BSS",
        "colab_type": "code",
        "outputId": "e33a4702-8ed7-4485-986f-f0c1d5f25db5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "def predict(text, model):\n",
        "  #input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
        "  #masks"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-58-951655275ba3>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    #masks\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ulTWaOr8QNY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = './model_save/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O0oYzj5nyfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model, \"Colabertmodel.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-tjHkR7lc1I",
        "colab_type": "text"
      },
      "source": [
        "Let's check out the file sizes, out of curiosity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqMzI3VTCZo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls -l --block-size=K ./model_save/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fr_bt2rFlgDn",
        "colab_type": "text"
      },
      "source": [
        "The largest file is the model weights, at around 418 megabytes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WUFUIQ8Cu8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls -l --block-size=M ./model_save/pytorch_model.h5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzGKvOFAll_e",
        "colab_type": "text"
      },
      "source": [
        "To save your model across Colab Notebook sessions, download it to your local machine, or ideally copy it to your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Trr-A-POC18_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount Google Drive to this Notebook instance.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxlZsafTC-V5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copy the model files to a directory in your Google Drive.\n",
        "!cp -r ./model_save/ \"./Revature\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}