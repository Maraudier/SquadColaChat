{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SQuADChatBotBertv1.5",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNPemQgx4htoAje6yKlKHwy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maraudier/SquadColaChat/blob/Maraudier-testing-1/SQuADChatBotBertv1_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkdZwaENGQll",
        "colab_type": "code",
        "outputId": "7d3e113b-5fc5-4192-e7c8-381127f51a35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "#Obtain your Kaggle token. \n",
        "token = {\"username\":\"lightshift\",\"key\":\"b7ab47deb22bd26d7cb071a39c232d1c\"}\n",
        "#Colab and Kaggle set up\n",
        "from google.colab import files\n",
        "!pip install kaggle\n",
        "!mkdir .kaggle\n",
        "!mkdir ~/.kaggle\n",
        "import json \n",
        "import os\n",
        "if not os.path.exists('/content/.kaggle/kaggle.json'):\n",
        "  with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)\n",
        "  !chmod 600 /content/.kaggle/kaggle.json\n",
        "  !cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json\n",
        "  !kaggle config set -n path -v{/content}\n",
        "\n",
        "  !kaggle datasets download -d stanfordu/stanford-question-answering-dataset -p /content\n",
        "  !unzip \\*.zip\n",
        "  !rm -rf /content/dataset/dataset/\n",
        "  !ls"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.6)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2019.11.28)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.21.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "mkdir: cannot create directory ‘.kaggle’: File exists\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzdGy8DQAQDW",
        "colab_type": "code",
        "outputId": "6f1d3bc3-c9cc-4abf-91b2-534d82a569d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "#Set up GPU utility and print confirmation\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name == '/device:GPU:0':\n",
        "  print('GPU confirmed: {}'.format(device_name))\n",
        "else:\n",
        "  raise SystemError(\"No GPU\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(\"%d GPU(s) \" % torch.cuda.device_count())\n",
        "  print(\"Using \", torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "  print(\"No GPU, defaulting to CPU\")\n",
        "  device = torch.device(\"cpu\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU confirmed: /device:GPU:0\n",
            "1 GPU(s) \n",
            "Using  Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pSUDYKKG6Ev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Obtain inputs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from subprocess import check_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNls-UtYuLlm",
        "colab_type": "code",
        "outputId": "6ea1acd1-f467-46d0-d99d-21c45056d7fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "#data management step 1\n",
        "squad_df = pd.read_json(\"train-v1.1.json\", orient='columns')\n",
        "#Only 2 columns, data includes nested json. More work to organize data \n",
        "squad_df.head()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data</th>\n",
              "      <th>version</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>{'title': 'University_of_Notre_Dame', 'paragra...</td>\n",
              "      <td>1.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>{'title': 'Beyoncé', 'paragraphs': [{'context'...</td>\n",
              "      <td>1.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>{'title': 'Montana', 'paragraphs': [{'context'...</td>\n",
              "      <td>1.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>{'title': 'Genocide', 'paragraphs': [{'context...</td>\n",
              "      <td>1.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>{'title': 'Antibiotics', 'paragraphs': [{'cont...</td>\n",
              "      <td>1.1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                data  version\n",
              "0  {'title': 'University_of_Notre_Dame', 'paragra...      1.1\n",
              "1  {'title': 'Beyoncé', 'paragraphs': [{'context'...      1.1\n",
              "2  {'title': 'Montana', 'paragraphs': [{'context'...      1.1\n",
              "3  {'title': 'Genocide', 'paragraphs': [{'context...      1.1\n",
              "4  {'title': 'Antibiotics', 'paragraphs': [{'cont...      1.1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnKok_qUxTzJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define a few functions to sort through the data further \n",
        "def squad_json_to_dataframe_train(input_file_path, record_path = ['data','paragraphs','qas','answers'], verbose = 1):\n",
        "  if verbose:\n",
        "    print(\"Reading json...\")\n",
        "  file = json.loads(open(input_file_path).read())\n",
        "  if verbose:\n",
        "    print(\"Working...\")\n",
        "  js = pd.io.json.json_normalize(file , record_path )\n",
        "  m = pd.io.json.json_normalize(file, record_path[:-1])\n",
        "  r = pd.io.json.json_normalize(file,record_path[:-2])\n",
        "  \n",
        "  idx = np.repeat(r['context'].values, r.qas.str.len())\n",
        "  ndx = np.repeat(m['id'].values, m['answers'].str.len())\n",
        "  m['context'] = idx\n",
        "  js['q_idx'] = ndx\n",
        "  main =pd.concat([m[['id', 'question' , 'context']].set_index('id'),js.set_index('q_idx')], 1, sort=False).reset_index()\n",
        "  main['c_id'] = main['context'].factorize()[0]\n",
        "  if verbose:\n",
        "    print(\"dataframe shape: {}\".format(main.shape))\n",
        "    print(\"Jobs Done\")\n",
        "  return main"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abGQqExs1g1X",
        "colab_type": "code",
        "outputId": "124c28a3-10af-4c22-ec4e-bf33e1d1a9dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "input_file_path = 'train-v1.1.json'\n",
        "record_path = ['data', 'paragraphs', 'qas', 'answers']\n",
        "train = squad_json_to_dataframe_train(input_file_path=input_file_path,record_path=record_path)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading json...\n",
            "Working...\n",
            "dataframe shape: (87599, 6)\n",
            "Jobs Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxNk1Yzl6V-g",
        "colab_type": "code",
        "outputId": "a1674793-c1e3-43cd-cd88-d3cf99c09fba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>text</th>\n",
              "      <th>c_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5733be284776f41900661182</td>\n",
              "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>515</td>\n",
              "      <td>Saint Bernadette Soubirous</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5733be284776f4190066117f</td>\n",
              "      <td>What is in front of the Notre Dame Main Building?</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>188</td>\n",
              "      <td>a copper statue of Christ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5733be284776f41900661180</td>\n",
              "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>279</td>\n",
              "      <td>the Main Building</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5733be284776f41900661181</td>\n",
              "      <td>What is the Grotto at Notre Dame?</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>381</td>\n",
              "      <td>a Marian place of prayer and reflection</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5733be284776f4190066117e</td>\n",
              "      <td>What sits on top of the Main Building at Notre...</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>92</td>\n",
              "      <td>a golden statue of the Virgin Mary</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      index  ... c_id\n",
              "0  5733be284776f41900661182  ...    0\n",
              "1  5733be284776f4190066117f  ...    0\n",
              "2  5733be284776f41900661180  ...    0\n",
              "3  5733be284776f41900661181  ...    0\n",
              "4  5733be284776f4190066117e  ...    0\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jZ0t5dkgl4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "question = train.question.values\n",
        "context = train.context.values\n",
        "text = train.text.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JE5l9IS03Aep",
        "colab_type": "code",
        "outputId": "e95b14bb-f88d-4497-d831-d3f7d4457d96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3Xhc5NfQbrJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForQuestionAnswering\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# question = train[['question']].copy()\n",
        "# text = train[['text']].copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blqIvQaQncdJ",
        "colab_type": "code",
        "outputId": "da3f8c44-7af1-40b3-9d43-c45646632ccf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "q_and_a = train[['question', 'text']].copy()\n",
        "q_and_a.head()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
              "      <td>Saint Bernadette Soubirous</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is in front of the Notre Dame Main Building?</td>\n",
              "      <td>a copper statue of Christ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
              "      <td>the Main Building</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What is the Grotto at Notre Dame?</td>\n",
              "      <td>a Marian place of prayer and reflection</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What sits on top of the Main Building at Notre...</td>\n",
              "      <td>a golden statue of the Virgin Mary</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question                                     text\n",
              "0  To whom did the Virgin Mary allegedly appear i...               Saint Bernadette Soubirous\n",
              "1  What is in front of the Notre Dame Main Building?                a copper statue of Christ\n",
              "2  The Basilica of the Sacred heart at Notre Dame...                        the Main Building\n",
              "3                  What is the Grotto at Notre Dame?  a Marian place of prayer and reflection\n",
              "4  What sits on top of the Main Building at Notre...       a golden statue of the Virgin Mary"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DrSNciAj9OyQ",
        "outputId": "da1a07db-9f2f-4bee-ea5d-4764cf51d171",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "#This Transformers package is a pytorch interface for working with BERT\n",
        "!pip install transformers"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgyuI6JBEwNb",
        "colab_type": "code",
        "outputId": "33e90ac2-2cc4-4669-bb43-e03188dad0fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#The Corpus of Linguistic Acceptability (CoLA) contains sets of sentances labled by their grammatical correctness\n",
        "!pip install wget\n",
        "import wget\n",
        "\n",
        "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
        "\n",
        "# Download the file (if we haven't already)\n",
        "if not os.path.exists('./cola_public_1.1.zip'):\n",
        "    wget.download(url, './cola_public_1.1.zip')\n",
        "\n",
        "if not os.path.exists('./cola_public_1.1.zip'):\n",
        "  wget.download(url, './cola_public_1.1.zip')\n",
        "\n",
        "if not os.path.exists('./cola_public/'):\n",
        "    !unzip cola_public_1.1.zip"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z6J2g1LZLYr",
        "colab_type": "code",
        "outputId": "2fc843e8-907d-4dee-ff7c-0d0c3f189b98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Label is the “acceptibility judgment” (0=unacceptable, 1=acceptable).\n",
        "\n",
        "cola_df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names = ['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "sentences = cola_df.sentence.values\n",
        "label =  cola_df.label.values\n",
        "\n",
        "#Visual Education\n",
        "print(\"Training samples: {:,}\\n\".format(cola_df.shape[0]))\n",
        "print(\"Example of label 0: Unacceptable sentences \")\n",
        "display(cola_df.loc[cola_df.label == 0].sample(5)[['sentence', 'label']])\n",
        "print(\"\\nExample of label 1: Acceptable sentences\")\n",
        "display(cola_df.loc[cola_df.label == 1].sample(5)[['sentence', 'label']])"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training samples: 8,551\n",
            "\n",
            "Example of label 0: Unacceptable sentences \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>It is important the more you eat, the more car...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1351</th>\n",
              "      <td>Reports of the lettering on the covers of whic...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3008</th>\n",
              "      <td>Dorothy needs the skills in her.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6792</th>\n",
              "      <td>All Mr Collins has done is have praised Lady d...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3540</th>\n",
              "      <td>They can down.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  label\n",
              "143   It is important the more you eat, the more car...      0\n",
              "1351  Reports of the lettering on the covers of whic...      0\n",
              "3008                   Dorothy needs the skills in her.      0\n",
              "6792  All Mr Collins has done is have praised Lady d...      0\n",
              "3540                                     They can down.      0"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Example of label 1: Acceptable sentences\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>The harder it has rained, how much faster a fl...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3938</th>\n",
              "      <td>The president looked weary.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7306</th>\n",
              "      <td>It was in the barn or it took place in the barn.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3059</th>\n",
              "      <td>Heather cabled the news.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5704</th>\n",
              "      <td>John and the man went to the store.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  label\n",
              "119   The harder it has rained, how much faster a fl...      1\n",
              "3938                        The president looked weary.      1\n",
              "7306   It was in the barn or it took place in the barn.      1\n",
              "3059                           Heather cabled the news.      1\n",
              "5704                John and the man went to the store.      1"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRTNpOopaQCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "# bert-base-uncased\n",
        "# 12-layer, 768-hidden, 12-heads, 110M parameters.\n",
        "# Trained on lower-cased English text.\n",
        "\n",
        "# bert-large-uncased\n",
        "# 24-layer, 1024-hidden, 16-heads, 340M parameters.\n",
        "# Trained on lower-cased English text.\n",
        "\n",
        "# bert-base-cased\n",
        "# 12-layer, 768-hidden, 12-heads, 110M parameters.\n",
        "# Trained on cased English text.\n",
        "\n",
        "# bert-large-cased\n",
        "# 24-layer, 1024-hidden, 16-heads, 340M parameters.\n",
        "# Trained on cased English text.\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4ct4w9hyKso",
        "colab_type": "code",
        "outputId": "dd701ff5-6472-4338-ea97-044ebbf97d3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "#https://huggingface.co/transformers/glossary.html\n",
        "#Visualize Cola\n",
        "print(\"\\nExample from Cola, showing token id's\")\n",
        "print(\"Original: \", sentences[0])\n",
        "print(\"Tokenized: \", tokenizer.tokenize(sentences[0]))\n",
        "print(\"Token IDs: \", tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))\n",
        "\n",
        "#Visualize BERT\n",
        "print(\"\\nExample from SQuAD, questions\")\n",
        "print(\"Original Question: \", question[0])\n",
        "print(\"Tokenized Question: \", tokenizer.tokenize(question[0]))\n",
        "print(\"Token ID's: \", tokenizer.convert_tokens_to_ids(tokenizer.tokenize(question[0])))\n",
        "\n",
        "print(\"\\nExample from SQuAD, answers\")\n",
        "print(\"Original Answer: \", text[0])\n",
        "print(\"Tokenized Answer: \", tokenizer.tokenize(text[0]))\n",
        "print(\"Token IDs: \", tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text[0])))\n",
        "\n",
        "print()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Example from Cola, showing token id's\n",
            "Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Tokenized:  ['our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
            "Token IDs:  [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]\n",
            "\n",
            "Example from SQuAD, questions\n",
            "Original Question:  To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
            "Tokenized Question:  ['to', 'whom', 'did', 'the', 'virgin', 'mary', 'allegedly', 'appear', 'in', '1858', 'in', 'lou', '##rdes', 'france', '?']\n",
            "Token ID's:  [2000, 3183, 2106, 1996, 6261, 2984, 9382, 3711, 1999, 8517, 1999, 10223, 26371, 2605, 1029]\n",
            "\n",
            "Example from SQuAD, answers\n",
            "Original Answer:  Saint Bernadette Soubirous\n",
            "Tokenized Answer:  ['saint', 'bern', '##ade', '##tte', 'so', '##ub', '##iro', '##us']\n",
            "Token IDs:  [3002, 16595, 9648, 4674, 2061, 12083, 9711, 2271]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDvSTJykqBvB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "12f19057-0af6-4007-ce45-8c9ba2d3168b"
      },
      "source": [
        "#CoLA input id_s \n",
        "input_ids = []\n",
        "for each_sent in sentences:\n",
        "  encoded_sentences = tokenizer.encode(each_sent, add_special_tokens = True) \n",
        "  input_ids.append(encoded_sentences)\n",
        "\n",
        "#Visual Education\n",
        "print('Original: ', sentences[0])\n",
        "print('TokenIDs: ', input_ids[0])"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "TokenIDs:  [101, 2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f15IN5xVeIJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_input_ids(sentences):\n",
        "  input_ids_list = []\n",
        "  for each_sent in sentences:\n",
        "    encoded_sentences = tokenizer.encode(each_sent, add_special_tokens = True)\n",
        "    input_ids_list.append(encoded_sentences)\n",
        "  return input_ids_list\n",
        "\n",
        "def attention_masks(input_ids):\n",
        "  attn_masks = []\n",
        "  for sent in input_ids:\n",
        "    attn_mask = [int(token_id > 0) for token_id in sent]\n",
        "    attn_masks.append(attn_mask)\n",
        "  return attn_masks\n",
        "\n",
        "question_ids = encode_input_ids(question)\n",
        "text_ids = encode_input_ids(text)\n",
        "#Tokenize question and text\n",
        "#Attention Masks on question and text\n",
        "#padding on question and text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwIwp_nvrRIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attn_question = attention_masks(question_ids)\n",
        "attn_text = attention_masks(text_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prRdS_z_rCdy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Add Padding for CoLA. Ensure size is equal across the board for all tensors. \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "MAX_LENGTH = max([len(sen) for sen in input_ids])\n",
        "input_ids = pad_sequences(input_ids, \n",
        "                          maxlen=MAX_LENGTH, \n",
        "                          dtype=\"long\", \n",
        "                          value=0, \n",
        "                          truncating=\"post\", \n",
        "                          padding=\"post\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbcdpWweQaTp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "9bd953bc-bc2d-4757-f372-c5a4a90483f1"
      },
      "source": [
        "\n",
        "A_MAX_LENGTH = max([len(sen) for sen in text_ids])\n",
        "Q_MAX_LENGTH = max([len(sen) for sen in question_ids])\n",
        "if Q_MAX_LENGTH > A_MAX_LENGTH:\n",
        "  QA_MAX_LENGTH = Q_MAX_LENGTH\n",
        "else: \n",
        "  QA_MAX_LENGTH = A_MAX_LENGTH\n",
        "print(\"Questions Max Length: {} \\nAnswers Max Length: {} \\nMax Length set to {}\".format(Q_MAX_LENGTH,  A_MAX_LENGTH, QA_MAX_LENGTH))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Questions Max Length: 63 \n",
            "Answers Max Length: 70 \n",
            "Max Length set to 70\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17Wg8CKZsv0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Padding for question text\n",
        "padded_question_ids = pad_sequences(question_ids,\n",
        "                                    maxlen=QA_MAX_LENGTH,\n",
        "                                    dtype=\"long\",\n",
        "                                    value=0,\n",
        "                                    truncating=\"post\",\n",
        "                                    padding=\"post\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aey89lfrtm12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Padding for answer text\n",
        "padded_text_ids = pad_sequences(question_ids,\n",
        "                                maxlen = QA_MAX_LENGTH,\n",
        "                                dtype=\"long\",\n",
        "                                value=0,\n",
        "                                truncating=\"post\",\n",
        "                                padding=\"post\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjI5LJ8ttQQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Atten masks helps ensure padding is recognized by the BERT model to be padding. \n",
        "attn_masks = []\n",
        "for sent in input_ids:\n",
        "  attn_mask = [int(token_id > 0) for token_id in sent]\n",
        "  attn_masks.append(attn_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71oWCfPguWEt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Train test split for SQuAD\n",
        "from sklearn.model_selection import train_test_split\n",
        "qtrain_in, qval_in, qtrain_label, qval_label = train_test_split(\n",
        "    question_ids,\n",
        "    text_ids,\n",
        "    random_state=88,\n",
        "    test_size=.7)\n",
        "qtrain_masks, qval_masks, _, _ = train_test_split(\n",
        "    attn_question,\n",
        "    attn_text,\n",
        "    random_state=88,\n",
        "    test_size=.7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4rCSieRt5iY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Train Test Split.for Cola\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n",
        "  input_ids, \n",
        "  label, \n",
        "  random_state=888, \n",
        "  test_size=.7)\n",
        "#same for masks, and blanks because we cant use train, test, spit into 3\n",
        "train_masks, val_masks, _, _ = train_test_split(\n",
        "    attn_masks, \n",
        "    label, \n",
        "    random_state=888, \n",
        "    test_size=.7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A_VqnZ-PMwm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "9bd66f70-746e-483b-b09a-a7fc12eb3bc8"
      },
      "source": [
        "#Good idea? Take a list, array, series, etc of non_tensor objects and make them all tensors.\n",
        "# def make_tensor(not_tensors):\n",
        "#   for each_non_tensor in not_tensors:\n",
        "#     tensor = torch.tensor(not_a_tensor)\n",
        "#   return tensor\n",
        "# train_test_set = [qtrain_in, qval_in, qtrain_label, qval_label, qtrain_masks, qval_masks]\n",
        "\n",
        "qtrain_in = torch.tensor(qtrain_in)  \n",
        "qval_in = torch.tensor(qval_in)\n",
        "qtrain_label = torch.tensor(qtrain_label)\n",
        "qval_label = torch.tensor(qval_label)\n",
        "qtrain_masks = torch.tensor(qtrain_masks)\n",
        "qval_masks = torch.tensor(qval_masks)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-22a1cd2fe5be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mqtrain_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqtrain_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mqval_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqval_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mqtrain_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqtrain_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mqval_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqval_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: expected sequence of length 10 at dim 1 (got 14)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0KPGBKfxYX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(val_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "\n",
        "validation_labels = torch.tensor(val_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(val_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUiI4W5W-f3L",
        "colab_type": "text"
      },
      "source": [
        "Got a little crazy here. Need to go back and revit these areas below in more depth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwDteI5LpEHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "batch_size = 32\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYYtpcQ4PAt3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#BERT for SQuAD\n",
        "from transformers import BertForQuestionAnswering, AdamW, BertConfig\n",
        "squad_model = BertForQuestionAnswering.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels = 2,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False)\n",
        "squad_model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j759KAlvLkDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels = 2,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False)\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIlryHDv8PtQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #Some options to fine tune with: \n",
        "#     # BertModel\n",
        "#     # BertForMaskedLM\n",
        "#     # BertForPreTraining\n",
        "#     # BertForQuestionAnswering\n",
        "#     # BertForTokenClassification\n",
        "#     # BertForNextSentencePrediction\n",
        "#     # BertForSequenceClassification "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxOEKsaH5dwu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwWTsLQIAZQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(squad_model.parameters())\n",
        "from transformers import get_linear_schedule_with_warmup\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rv_UehFp5jkg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "epochs = 4\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8xYvMSb-aeK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bu6jHYUm-24Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6ZLN_T0_U0_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "seed_val = 38\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "loss_values = []\n",
        "for epoch_i in range(0, epochs):\n",
        "  print(\"|-|\")\n",
        "  print('=Epoch {:}/{:}'.format(epoch_i + 1, epochs))\n",
        "  print(\"Training...stand by\")\n",
        "\n",
        "  t0 = time.time()\n",
        "\n",
        "  total_loss = 0\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    if step % 40 == 0 and not step == 0:\n",
        "      elapsed = format_time(time.time() - t0)\n",
        "      print('Batch {:>5,} of {:>5,}. Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "    b_in_ids = batch[0].to(device)\n",
        "    b_in_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "\n",
        "    model.zero_grad()\n",
        "\n",
        "    outputs = model(b_in_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_in_mask, \n",
        "                    labels=b_labels)\n",
        "    \n",
        "    loss = outputs[0]\n",
        "    total_loss += loss.item()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "  avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "  loss_values.append(avg_train_loss)\n",
        "\n",
        "  print(\"|-|\")\n",
        "  print(\"Avg train loss: {0:.2f}\".format(avg_train_loss))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAOelg8mEMYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPsxQz9cEeU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LENGTH, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XogzERT_EnHj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osk9Dm32EyAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t49mMWRmEz9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "  matthews_set.append(matthews)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPy15dzfE-X6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matthews_set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgIHOufeFEd5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAIxLorxFSpg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = './model_save/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ac0SAT-FceR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Mount Google Drive to this Notebook instance.\n",
        "# from google.colab import drive\n",
        "#     drive.mount('/content/drive')\n",
        "#!cp -r ./model_save/ \"./drive/Shared drives/ChrisMcCormick.AI/Blog Posts/BERT Fine-Tuning/\"\n",
        "# Load a trained model and vocabulary that you have fine-tuned\n",
        "# model = model_class.from_pretrained(output_dir)\n",
        "# tokenizer = tokenizer_class.from_pretrained(output_dir)\n",
        "\n",
        "# # Copy the model to the GPU.\n",
        "# model.to(device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}